# Update config.yaml file :
: it will return evaluation matrix

model_evaluation:
  root_dir: artifacts/model_evaluation
  data_path: artifacts/data_transformation/samsum_dataset
  model_path: artifacts/model_trainer/pegasus_samsum_model
  tokenizer_name: artifacts/model_trainer/tokenizer
  metric_file_name: artifacts/model_evaluation/metrics.csv
  

# Preparing entity (research)

from dataclasses import dataclass
from pathlib import Path

@dataclass(frozen=True)
class ModelEvaluationConfig:
    root_dir: Path
    data_path: Path
    model_path: Path
    tokenizer_path: Path
    metric_file_name: Path



# Prepare the configuration manager :
 
 before that import some libraries:

from textsummarizer.constants import *
from textsummarizer.utils.common import read_yml,create_directories


# inside it add a method get_model_evaluation_config :
 class ConfigurationManager:
    def __init__(self,
        config_filepath=CONFIG_FILE_PATH,
        params_filepath=PARAMS_FILE_PATH):

        self.config=read_yml(config_filepath)
        self.params=read_yml(params_filepath)

        create_directories([self.config.artifacts_root])
    
    def get_model_evaluation_config(self) -> ModelEvaluationConfig:
        config = self.config.model_evaluation

        create_directories([config.root_dir])

        model_evaluation_config=ModelEvaluationConfig(
            root_dir=config.root_dir,
            data_path=config.data_path,
            model_path=config.model_path,
            tokenizer_path=config.tokenizer_path,
            metric_file_name=config.metric_file_name
        )

        return model_evaluation_config


# Create components :
import some libraries:
from transformers import AutoModelForSeq2SeqLM,AutoTokenizer
from datasets import load_dataset,load_from_disk
import evaluate
import torch
import pandas as pd
from tqdm import tqdm



# Define model evaluation class : 
class ModelEvaluation:

    def __init__(self,config: ModelEvaluationConfig):
        self.config = config

    def generate_batch_sized_chunks(list_of_elements, batch_size):
        """
        Split the dataset into smaller batches that we can process simultaneously
        Yield successive batch-sized chunks from list_of_elements
        
        """
        for i in range(0,len(list_of_elements),batch_size):
            yield list_of_elements[i:i+batch_size]


    def calculate_metric_on_test_ds(dataset,metric,model,tokenizer,
                                    batch_size=16,device=device,
                                    column_text="article",
                                    column_summary="highlights"):
        article_batches= list(generate_batch_sized_chunks(dataset[column_text],batch_size))
        target_batches= list(generate_batch_sized_chunks(dataset[column_summary],batch_size))

        for article_batch, target_batches in tqdm(
            zip(article_batches,target_batches), total=len(article_batches)):

            inputs= tokenizer(article_batch,max_length=1024,truncation=True,
                              padding="max_length",return_tensors="pt")
            
            summaries= model.generate(input_ids=inputs["input_ids"].to(device),
                                      attention_mask=inputs["attention_mask"].to(device),
                                      length_penalty=0.8,num_beams=8,max_length=128)
            ''' parameter for length penalty sensures that the model does not generate sequence that are too long.'''

            # Finally, we decode the generate textS,
            # replace the token, and the decoded texts with the refernce to the metric.
            decode_summaries=[tokenizer.decode(s,skip_special_tokens=True,
                                    clean_up_tokenization_spaces=True)
                                for s in summaries]
            
            decode_summaries=[d.replace(""," ") for d in decode_summaries]

            metric.add_batch(predictions=decode_summaries,refernces=target_batch)
        

        # finally compute and return ROUGE scores
        score= evaluate.compute()
        return score

            
# added 2 more methods and these 2 calling in our evaluate

    def evaluate_score(self):
        # device="cuda" if torch.cuda.is_available() else "cpu"
        device="cpu"
        tokenizer= AutoTokenizer.from_pretrained(self.config.tokenizer_path)
        model_pegasus= AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path).to(device)

        # loading data
        dataset_samsum_pt= load_from_disk(self.config.data_path)

        : here : loading our tokenizer,model and then calculating rouge matrix




# Modular coding

1. Update Entity :

@dataclass(frozen=True)
class ModelEvaluationConfig:
    root_dir: Path
    data_path: Path
    model_path: Path
    tokenizer_path: Path
    metric_file_name: Path


2. Update the configuration (writing the method):
from textsummarizer.entity import ModelTrainerConfig,ModelEvaluationConfig
def get_model_evaluation_config(self) -> ModelEvaluationConfig:
        config = self.config.model_evaluation

        create_directories([config.root_dir])

        model_evaluation_config=ModelEvaluationConfig(
            root_dir=config.root_dir,
            data_path=config.data_path,
            model_path=config.model_path,
            tokenizer_path=config.tokenizer_name,
            metric_file_name=config.metric_file_name
        )

        return model_evaluation_config

3. Create the components :
(create a new file : model_evaluation)
importing the packages :
from transformers import AutoModelForSeq2SeqLM,AutoTokenizer
from datasets import load_dataset,load_from_disk
import evaluate
import torch
import pandas as pd
from tqdm import tqdm


also req. model evaluation config:
from textsummarizer.entity import ModelEvaluationConfig



class ModelEvaluation:
    def __init__(self, config: ModelEvaluationConfig):
        self.config = config


    
    def generate_batch_sized_chunks(self,list_of_elements, batch_size):
        """split the dataset into smaller batches that we can process simultaneously
        Yield successive batch-sized chunks from list_of_elements."""
        for i in range(0, len(list_of_elements), batch_size):
            yield list_of_elements[i : i + batch_size]

    #device="cuda" if torch.cuda.is_available() else "cpu"
    def calculate_metric_on_test_ds(self,dataset, metric, model, tokenizer, 
                               batch_size=16, device="cpu", 
                               column_text="article", 
                               column_summary="highlights"):
        article_batches = list(self.generate_batch_sized_chunks(dataset[column_text], batch_size))
        target_batches = list(self.generate_batch_sized_chunks(dataset[column_summary], batch_size))

        for article_batch, target_batch in tqdm(
            zip(article_batches, target_batches), total=len(article_batches)):
            
            inputs = tokenizer(article_batch, max_length=1024,  truncation=True, 
                            padding="max_length", return_tensors="pt")
            
            summaries = model.generate(input_ids=inputs["input_ids"].to(device),
                            attention_mask=inputs["attention_mask"].to(device), 
                            length_penalty=0.8, num_beams=8, max_length=128)
            ''' parameter for length penalty ensures that the model does not generate sequences that are too long. '''
            
            # Finally, we decode the generated texts, 
            # replace the  token, and add the decoded texts with the references to the metric.
            decoded_summaries = [tokenizer.decode(s, skip_special_tokens=True, 
                                    clean_up_tokenization_spaces=True) 
                for s in summaries]      
            
            decoded_summaries = [d.replace("", " ") for d in decoded_summaries]
            
            
            metric.add_batch(predictions=decoded_summaries, references=target_batch)
            
        #  Finally compute and return the ROUGE scores.
        score = metric.compute()
        return score


    def evaluate(self):
        # device = "cuda" if torch.cuda.is_available() else "cpu"
        device=torch.device("cpu")
        tokenizer = AutoTokenizer.from_pretrained(self.config.tokenizer_path)
        model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(self.config.model_path).to(device)
       
        #loading data 
        dataset_samsum_pt = load_from_disk(self.config.data_path)


        rouge_names = ["rouge1", "rouge2", "rougeL", "rougeLsum"]
  
        rouge_metric = evaluate.load('rouge')

        score = self.calculate_metric_on_test_ds(
        dataset_samsum_pt['train'][0:10], rouge_metric, model_pegasus, tokenizer, batch_size = 2, column_text = 'dialogue', column_summary= 'summary'
            )

        rouge_dict = {rn: score[rn] for rn in rouge_names}


        df = pd.DataFrame(rouge_dict, index = ['pegasus'] )
        df.to_csv(self.config.metric_file_name, index=False)




4. Writing the pipeline :
(create a new file stage_05_model_evalution.py)
importing some libraries and create modelevalutionpipeline :
from textsummarizer.config.configuration import ConfigurationManager
from textsummarizer.components.model_evaluation import ModelEvaluation
from textsummarizer.logging import logger
import os
os.environ["CUDA_VISIBLE_DEVICES"] = ""
class ModelEvaluatinTrainingPipeline:
    def __init__(self):
        pass

    def main(self):
        config = ConfigurationManager()
        model_evaluation_config= config.get_model_evaluation_config()
        model_evaluation_config=ModelEvaluation(config=model_evaluation_config)
        model_evaluation_config.evaluate()


5. Now adding it into main.py
import model_evalution pipeline
from textsummarizer.pipeline.stage_05_model_evaluation import ModelEvaluatinTrainingPipeline

STAGE_NAME= "Model Evaluation Stage" 

try:
    logger.info(f">>>>>>> stage {STAGE_NAME} started <<<<<<<")
    model_trainer=ModelEvaluatinTrainingPipeline()
    model_trainer.main()
    logger.info(f">>>>>>> stage {STAGE_NAME} completed <<<<<<<<<\n\nx===========x")
except Exception as e:
    logger.exception(e)
    raise e
