# write config.yaml file


  data_transformation:
    root_dir: artifacts/data_transformation # inside save it transformed data
    data_path: artifacts/data_ingestion/samsum_dataset 
    tokenizer_name: google/pegasus-cnn_dailymail



# writing the entity :
from dataclasses import dataclass
from pathlib import Path

@dataclass(frozen=True)
class DataTransformationConfig:
    root_dir: Path # save the transformed data here
    data_path: Path # here access the data
    tokenizer_name: Path  # from here it will download the model



# Create the configuarion Manager:

inside it create a method : get_data_transformation_confg : it will return only 3 things.
class ConfigurationManager:
    def __init__(self,
        config_filepath=CONFIG_FILE_PATH,
        params_filepath=PARAMS_FILE_PATH): # copied

        self.config=read_yml(config_filepath)
        self.params=read_yml(params_filepath)

        create_directories([self.config.artifacts_root])

    def get_data_transformation_config(self) ->DataTransformationConfig:
        config= self.config.data_transformation

        create_directories([config.root_dir])

        data_transformation_config=DataTransformationConfig(
            root_dir=config.root_dir,   # created
            data_path=config.data_path,
            tokenizer_name=config.tokenizer_name
        )

        return data_transformation_config




# Create the components:

# Do some import operation : 
from textsummarizer.logging import logger
from transformers import AutoTokenizer
from datasets import load_dataset, load_from_disk



# first initialize the class : i.e config and tokenizer
# secondly add method convert example into features:
# third method : it will start the data_transformation
class DataTransformation:

    def __init__(self,config : DataTransformationConfig):
        self.config=config
        self.tokenizer=AutoTokenizer.from_pretrained(config.tokenizer_name)
    

    def conver_examples_into_features(self,example_batch):
        input_encodings=self.tokenizer(example_batch['dialogue'],max_length=1024,truncation=True)

        with self.tokenizer.as_target_tokenizer():
            target_encodings=self.tokenizer(example_batch['summary'],max_length=128,truncation=True)

        return {
            'input_ids': input_encodings['input_ids'],
            'attention_mask': input_encodings['attention_mask'],
            'labels':target_encodings['input_ids']
        }

        def convert(self):
        dataset_samsum=load_from_disk(self.config.data_path)
        dataset_samsum_pt=dataset_samsum.map(self.conver_examples_into_features,batch=True)
        dataset_samsum_pt.save_to_disk(os.path.join(self.config.root_dir,"samsum_dataset")) # saving data here in data_transformation after data transformation.



# Create the pipeline :

try:
    config=ConfigurationManager()
    data_transformation_config=config.get_data_transformation_config()
    data_transformation=DataTransformation(config=data_transformation_config)
    data_transformation.convert()
except Exception as e:
    raise e



# Convert it into modular coding

1. copy the entity :
2. copy the ConfigurationManager into src/config

def get_data_transformation_config(self) ->DataTransformationConfig:
        config= self.config.data_transformation

        create_directories([config.root_dir])

        data_transformation_config=DataTransformationConfig(
            root_dir=config.root_dir,
            data_path=config.data_path,
            tokenizer_name=config.tokenizer_name
        )

        return data_transformation_config

3.  Create a component : (create a new file in coponents :data_transformation)
copy the libraries and class
import os
from textsummarizer.logging import logger
from transformers import AutoTokenizer
from datasets import load_dataset, load_from_disk
from textsummarizer.entity import DataTransformationConfig

class DataTransformation:

    def __init__(self,config : DataTransformationConfig):
        self.config=config
        self.tokenizer=AutoTokenizer.from_pretrained(config.tokenizer_name)

    def conver_examples_into_features(self,example_batch):
        input_encodings=self.tokenizer(example_batch['dialogue'],max_length=1024,truncation=True)

        with self.tokenizer.as_target_tokenizer():
            target_encodings=self.tokenizer(example_batch['summary'],max_length=128,truncation=True)

        return {
            'input_ids': input_encodings['input_ids'],
            'attention_mask': input_encodings['attention_mask'],
            'labels':target_encodings['input_ids']
        }
    
    def convert(self):
        dataset_samsum=load_from_disk(self.config.data_path)
        dataset_samsum_pt=dataset_samsum.map(self.conver_examples_into_features,batched=True)
        dataset_samsum_pt.save_to_disk(os.path.join(self.config.root_dir,"samsum_dataset"))




4. Create a pipeline :
(create a new file : stage_03_data_transformation.py)
from textsummarizer.config.configuration import ConfigurationManager
from textsummarizer.components.data_transformation import DataTransformation
from textsummarizer.logging import logger

class DataValidationTrainingPipeline:
    def __init__(self):
        pass
 
    def main(self):
        config=ConfigurationManager() # copied and changes name (from stage 2)
    data_transformation_config=config.get_data_transformation_config()
    data_transformation=DataTransformation(config=data_transformation_config)
    data_transformation.convert()



5. Call this thing in main.py

STAGE_NAME= "Data Transformation Stage" # giving stage name(stage of data ingestion)

try:
    logger.info(f">>>>>>> stage {STAGE_NAME} started <<<<<<<")
    data_transformation=DataTransformationTrainingPipeline()
    data_transformation.main()
    logger.info(f">>>>>>> stage {STAGE_NAME} completed <<<<<<<<<\n\nx===========x")
except Exception as e:
    logger.exception(e)
    raise e